{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0e08b8",
   "metadata": {},
   "source": [
    ">Use esse notebook para comparar os resultados dos testes. Graficos s√£o gerados ao final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# carregar m√©tricas\n",
    "metric_files = glob.glob(\"../results/*_metrics.csv\")\n",
    "dfs = [pd.read_csv(f) for f in metric_files]\n",
    "metrics = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "quality_files = glob.glob(\"../results/*_quality.csv\")\n",
    "qdfs = [pd.read_csv(f) for f in quality_files]\n",
    "quality = pd.concat(qdfs, ignore_index=True)\n",
    "\n",
    "# agrega por modelo\n",
    "agg = metrics.groupby(\"model_id\").agg(\n",
    "    latency_mean=(\"latency_s\", \"mean\"),\n",
    "    latency_std=(\"latency_s\", \"std\"),\n",
    "    tokens_per_s_mean=(\"tokens_per_s\", \"mean\"),\n",
    "    vram_gb_peak_mean=(\"vram_gb_peak\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "plt.bar(agg[\"model_id\"], agg[\"latency_mean\"], yerr=agg[\"latency_std\"], capsize=4)\n",
    "plt.title(\"Lat√™ncia m√©dia ¬± desvio padr√£o (s)\")\n",
    "\n",
    "agg[\"score_efficiency\"] = agg[\"tokens_per_s_mean\"] / agg[\"vram_gb_peak_mean\"]\n",
    "agg[\"score_global\"] = (\n",
    "    0.4*(1/agg[\"latency_mean\"]) +\n",
    "    0.3*agg[\"tokens_per_s_mean\"] +\n",
    "    0.3*agg[\"quality_score\"]\n",
    ")\n",
    "agg.sort_values(\"score_global\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "cols = [\"latency_mean\",\"tokens_per_s_mean\",\"quality_score\",\"vram_gb_peak_mean\"]\n",
    "labels = [\"Lat√™ncia (‚Üì)\", \"Tokens/s (‚Üë)\", \"Qualidade (‚Üë)\", \"VRAM (‚Üì)\"]\n",
    "values = agg.loc[agg[\"model_id\"]==\"microsoft/Phi-3-mini-4k-instruct\", cols].values.flatten()\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, len(cols), endpoint=False).tolist()\n",
    "values = np.concatenate((values, [values[0]]))\n",
    "angles += angles[:1]\n",
    "\n",
    "plt.polar(angles, values)\n",
    "plt.fill(angles, values, alpha=0.25)\n",
    "plt.title(\"Perfil de desempenho ‚Äî Phi-3 Mini\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b181ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Compare & Plot Benchmark Results ===\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "\n",
    "# ---------- Load Metrics ----------\n",
    "metric_files = glob.glob(\"../results/*_metrics.csv\")\n",
    "quality_files = glob.glob(\"../results/*_quality.csv\")\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in metric_files]\n",
    "metrics = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "qdfs = [pd.read_csv(f) for f in quality_files]\n",
    "quality = pd.concat(qdfs, ignore_index=True)\n",
    "\n",
    "# ---------- Aggregate ----------\n",
    "agg = metrics.groupby(\"model_id\").agg(\n",
    "    latency_s_mean=(\"latency_s\", \"mean\"),\n",
    "    tokens_per_s_mean=(\"tokens_per_s\", \"mean\"),\n",
    "    vram_gb_peak_mean=(\"vram_gb_peak\", \"max\"),\n",
    "    new_tokens_mean=(\"new_tokens\", \"mean\"),\n",
    ").reset_index()\n",
    "\n",
    "agg = agg.merge(quality[[\"model_id\", \"quality_score\"]], on=\"model_id\", how=\"left\")\n",
    "\n",
    "agg = agg.sort_values(by=\"quality_score\", ascending=False).reset_index(drop=True)\n",
    "agg[\"efficiency\"] = agg[\"quality_score\"] * agg[\"tokens_per_s_mean\"] / agg[\"vram_gb_peak_mean\"]\n",
    "\n",
    "print(\"Resumo consolidado:\\n\")\n",
    "display(agg.style.background_gradient(subset=[\"quality_score\", \"tokens_per_s_mean\", \"efficiency\"], cmap=\"Greens\"))\n",
    "\n",
    "# ---------- Gr√°ficos ----------\n",
    "def label_rotation():\n",
    "    plt.xticks(rotation=25, ha=\"right\")\n",
    "\n",
    "# Lat√™ncia\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(agg[\"model_id\"], agg[\"latency_s_mean\"])\n",
    "plt.title(\"‚è±Ô∏è Lat√™ncia m√©dia por modelo (s)\")\n",
    "plt.ylabel(\"Segundos\")\n",
    "label_rotation()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tokens/s\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(agg[\"model_id\"], agg[\"tokens_per_s_mean\"])\n",
    "plt.title(\"‚ö° Tokens gerados por segundo\")\n",
    "plt.ylabel(\"tokens/s\")\n",
    "label_rotation()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# VRAM\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(agg[\"model_id\"], agg[\"vram_gb_peak_mean\"])\n",
    "plt.title(\"üíæ Pico de VRAM (GB)\")\n",
    "plt.ylabel(\"GB\")\n",
    "label_rotation()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Qualidade\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(agg[\"model_id\"], agg[\"quality_score\"])\n",
    "plt.title(\"üéØ Qualidade m√©dia (score math)\")\n",
    "plt.ylabel(\"Score 0‚Äì1\")\n",
    "label_rotation()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Efici√™ncia global\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(agg[\"model_id\"], agg[\"efficiency\"])\n",
    "plt.title(\"üìà Efici√™ncia: (Qualidade √ó Tokens/s) √∑ VRAM\")\n",
    "plt.ylabel(\"√çndice relativo\")\n",
    "label_rotation()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trade-off scatter\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(agg[\"vram_gb_peak_mean\"], agg[\"quality_score\"], s=150)\n",
    "for i, row in agg.iterrows():\n",
    "    plt.annotate(row[\"model_id\"].split(\"/\")[-1], (row[\"vram_gb_peak_mean\"], row[\"quality_score\"]))\n",
    "plt.title(\"VRAM vs Qualidade\")\n",
    "plt.xlabel(\"VRAM (GB)\")\n",
    "plt.ylabel(\"Qualidade (0‚Äì1)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Radar chart\n",
    "labels = [\"Qualidade\", \"Tokens/s\", \"Lat√™ncia (invertida)\", \"VRAM (invertida)\"]\n",
    "models = agg[\"model_id\"].tolist()\n",
    "\n",
    "# Normaliza√ß√£o 0‚Äì1\n",
    "norm = lambda x: (x - x.min()) / (x.max() - x.min() + 1e-9)\n",
    "quality_norm = norm(agg[\"quality_score\"])\n",
    "tokens_norm = norm(agg[\"tokens_per_s_mean\"])\n",
    "latency_norm = 1 - norm(agg[\"latency_s_mean\"])\n",
    "vram_norm = 1 - norm(agg[\"vram_gb_peak_mean\"])\n",
    "\n",
    "# Monta matriz (n_features x n_models)\n",
    "data = np.vstack([quality_norm, tokens_norm, latency_norm, vram_norm])\n",
    "\n",
    "# √Çngulos para cada m√©trica\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False)\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    values = np.concatenate((data[:, i], [data[0, i]]))\n",
    "    ax.plot(angles, values, label=model.split(\"/\")[-1])\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels)\n",
    "plt.title(\"üìä Comparativo global ‚Äî desempenho relativo\", pad=20)\n",
    "plt.legend(bbox_to_anchor=(1.25, 1.0))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
