{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946b29c9",
   "metadata": {},
   "source": [
    "> As 3 primeiras celulas são comuns a todos os notebooks de testes  \n",
    "**Criar uma venv, e depois instalar os requirements do arquivo requirements.txt.**  \n",
    "**CUDA versão 12.8**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a5307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1024**3, 2))\n",
    "\n",
    "def load_bnb_4bit(model_id: str):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "def format_chat_inst(model_family: str, system: str, user: str):\n",
    "    if model_family == \"mistral\":\n",
    "        return f\"<s>[INST] <<SYS>>\\n{system}\\n<</SYS>>\\n{user} [/INST]\"\n",
    "    elif model_family == \"gemma\":\n",
    "        return f\"<start_of_turn>system\\n{system}<end_of_turn>\\n<start_of_turn>user\\n{user}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    elif model_family == \"phi3\":\n",
    "        return f\"<s>[INST] {system}\\n\\n{user} [/INST]\"\n",
    "    else:\n",
    "        return user  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_and_measure(tok, model, prompt, max_new_tokens=750, temperature=0.7, top_p=0.9):\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    txt = tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # mede VRAM pico\n",
    "    vram_peak = None\n",
    "    if device == \"cuda\":\n",
    "        vram_peak = torch.cuda.max_memory_allocated() / (1024**3)  # GB\n",
    "\n",
    "    # tokens gerados\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    tps = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "\n",
    "    return {\"text\": txt, \"latency_s\": dt, \"new_tokens\": new_tokens, \"tokens_per_s\": tps, \"vram_gb_peak\": vram_peak}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5cdbb4",
   "metadata": {},
   "source": [
    "**Lembre-se de pegar o token da hugging face no site e adicionar abaixo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee00014",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.environ[\"HF_TOKEN\"] = \"\"\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tok, model = load_bnb_4bit(MODEL_ID)\n",
    "model_family = \"mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb194f53",
   "metadata": {},
   "source": [
    "**Teste básico de aquecimento:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ca242",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"Você é um assistente sênior, responda em pt-BR.\"\n",
    "user = \"Liste 4 cuidados ao comparar LLMs locais.\"\n",
    "prompt = format_chat_inst(model_family, system, user)\n",
    "res = generate_and_measure(tok, model, prompt)\n",
    "print(res[\"text\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/prompts_baseline.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "rows = []\n",
    "system = \"Responda em pt-BR, com precisão e sem enrolação.\"\n",
    "for p in tqdm(prompts):\n",
    "    prompt = format_chat_inst(model_family, system, p)\n",
    "    r = generate_and_measure(tok, model, prompt, max_new_tokens=200)\n",
    "    rows.append({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"prompt\": p,\n",
    "        \"latency_s\": r[\"latency_s\"],\n",
    "        \"tokens_per_s\": r[\"tokens_per_s\"],\n",
    "        \"vram_gb_peak\": r[\"vram_gb_peak\"],\n",
    "        \"new_tokens\": r[\"new_tokens\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(f\"../results/{MODEL_ID.split('/')[-1]}_metrics.csv\", index=False)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aee389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import pandas as pd\n",
    "\n",
    "NUMBER_FULL = re.compile(r\"^\\s*-?\\d+(?:[.,]\\d+)?\\s*$\")\n",
    "NUMBER_ANY  = re.compile(r\"-?\\d+(?:[.,]\\d+)?\")\n",
    "\n",
    "def is_number_string(s: str) -> bool:\n",
    "    \"\"\"Retorna True se a string é um número (int/float) puro.\"\"\"\n",
    "    return bool(NUMBER_FULL.match(str(s)))\n",
    "\n",
    "def extract_number(text: str):\n",
    "    \"\"\"Extrai o primeiro número (int/float) do texto; retorna float ou None.\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    m = NUMBER_ANY.search(str(text))\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0).replace(\",\", \".\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normaliza texto para comparação exata não numérica.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(s or \"\")).strip().lower()\n",
    "\n",
    "def ask_and_score_math(\n",
    "    items,\n",
    "    *,\n",
    "    system_prompt=\"Responda somente com a resposta final.\",\n",
    "    max_new_tokens=64,\n",
    "    tol=1e-2,\n",
    "    verbose=False,\n",
    "):\n",
    "    logs = []\n",
    "    acertos = 0\n",
    "\n",
    "    for idx, item in enumerate(items, 1):\n",
    "        q = item.get(\"q\", \"\")\n",
    "        a = str(item.get(\"a\", \"\")).strip()\n",
    "\n",
    "        prompt = format_chat_inst(model_family, system_prompt, q)\n",
    "\n",
    "        out = generate_and_measure(tok, model, prompt, max_new_tokens=max_new_tokens)\n",
    "        model_text = (out.get(\"text\") or \"\").strip()\n",
    "\n",
    "        if is_number_string(a):\n",
    "            gold = float(a.replace(\",\", \".\"))\n",
    "            pred = extract_number(model_text)\n",
    "\n",
    "            correct = False\n",
    "            if pred is not None:\n",
    "                try:\n",
    "                    correct = abs(pred - gold) <= tol\n",
    "                except Exception:\n",
    "                    correct = False\n",
    "\n",
    "            if not correct:\n",
    "                txt_norm = normalize_text(model_text)\n",
    "                correct = normalize_text(a) in txt_norm or str(int(gold)) in txt_norm\n",
    "\n",
    "        else:\n",
    "            correct = normalize_text(model_text) == normalize_text(a)\n",
    "\n",
    "        if verbose and not correct:\n",
    "            print(f\"[❌] Q: {q}\")\n",
    "            print(f\"    Pred: {model_text}\")\n",
    "            print(f\"    Gold: {a}\")\n",
    "\n",
    "        acertos += int(correct)\n",
    "\n",
    "        logs.append({\n",
    "            \"idx\": idx,\n",
    "            \"question\": q,\n",
    "            \"gold\": a,\n",
    "            \"prediction\": model_text,\n",
    "            \"correct\": int(correct),\n",
    "            \"latency_s\": out.get(\"latency_s\"),\n",
    "            \"tokens_per_s\": out.get(\"tokens_per_s\"),\n",
    "            \"vram_gb_peak\": out.get(\"vram_gb_peak\"),\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{idx}] OK={correct} | Q: {q}\\n  pred: {model_text}\\n  gold: {a}\\n\")\n",
    "\n",
    "    total = len(items) or 1\n",
    "    acc = acertos / total\n",
    "\n",
    "    summary = {\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"n\": len(items),\n",
    "        \"acertos\": acertos,\n",
    "        \"accuracy\": acc,\n",
    "        \"latency_avg_s\": pd.Series([r[\"latency_s\"] for r in logs]).mean(),\n",
    "        \"tps_avg\": pd.Series([r[\"tokens_per_s\"] for r in logs]).mean(),\n",
    "        \"vram_peak_max_gb\": pd.Series([r[\"vram_gb_peak\"] for r in logs]).max(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(logs)\n",
    "    return summary, df\n",
    "\n",
    "with open(\"../data/qa_math_eval.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    math_items = json.load(f)   # lista de {\"q\",\"a\"}\n",
    "\n",
    "summary, df_logs = ask_and_score_math(math_items, tol=1e-2, verbose=True)\n",
    "\n",
    "print(\"== Resumo Math ==\")\n",
    "print({\n",
    "    \"model_id\": summary[\"model_id\"],\n",
    "    \"n\": summary[\"n\"],\n",
    "    \"acertos\": summary[\"acertos\"],\n",
    "    \"accuracy\": round(summary[\"accuracy\"], 3),\n",
    "    \"latency_avg_s\": round(float(summary[\"latency_avg_s\"] or 0), 3),\n",
    "    \"tps_avg\": round(float(summary[\"tps_avg\"] or 0), 2),\n",
    "    \"vram_peak_max_gb\": round(float(summary[\"vram_peak_max_gb\"] or 0), 2),\n",
    "})\n",
    "\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "\n",
    "per_model_slug = MODEL_ID.split(\"/\")[-1]\n",
    "df_logs.to_csv(f\"../results/{per_model_slug}_math_logs.csv\", index=False)\n",
    "\n",
    "dfq = pd.DataFrame([{\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"quality_math\": summary[\"accuracy\"],\n",
    "    \"quality_fact\": None,\n",
    "    \"quality_score\": summary[\"accuracy\"]\n",
    "}])\n",
    "dfq.to_csv(f\"../results/{per_model_slug}_quality.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
