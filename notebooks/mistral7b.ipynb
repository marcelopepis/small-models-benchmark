{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976a5307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\small-models-benchmark\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "VRAM (GB): 23.99\n"
     ]
    }
   ],
   "source": [
    "# utils comuns\n",
    "import os, time, json, re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1024**3, 2))\n",
    "\n",
    "def load_bnb_4bit(model_id: str):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "def format_chat_inst(model_family: str, system: str, user: str):\n",
    "    \"\"\"formata o prompt de acordo com o modelo.\"\"\"\n",
    "    if model_family == \"mistral\":\n",
    "        return f\"<s>[INST] <<SYS>>\\n{system}\\n<</SYS>>\\n{user} [/INST]\"\n",
    "    elif model_family == \"gemma\":\n",
    "        return f\"<start_of_turn>system\\n{system}<end_of_turn>\\n<start_of_turn>user\\n{user}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    elif model_family == \"phi3\":\n",
    "        # phi-3 segue estilo chat simples; esse formato funciona bem\n",
    "        return f\"<s>[INST] {system}\\n\\n{user} [/INST]\"\n",
    "    else:\n",
    "        return user  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eac8639",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_and_measure(tok, model, prompt, max_new_tokens=750, temperature=0.7, top_p=0.9):\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # sincroniza antes e depois para medir latência corretamente\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    txt = tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # mede VRAM pico\n",
    "    vram_peak = None\n",
    "    if device == \"cuda\":\n",
    "        vram_peak = torch.cuda.max_memory_allocated() / (1024**3)  # GB\n",
    "\n",
    "    # tokens gerados\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    tps = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "\n",
    "    return {\"text\": txt, \"latency_s\": dt, \"new_tokens\": new_tokens, \"tokens_per_s\": tps, \"vram_gb_peak\": vram_peak}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee00014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "token = os.environ[\"HF_TOKEN\"] = \"\"\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tok, model = load_bnb_4bit(MODEL_ID)\n",
    "model_family = \"mistral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "514ca242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "Você é um assistente sênior, responda em pt-BR.\n",
      "<</SYS>>\n",
      "Liste 4 cuidados ao comparar LLMs locais. [/INST] I. Comparar o perfil de especialização: Verifique se os LLMs oferecidos pelas universidades atendem às suas necessidades específicas e interesses acadêmicos. Alguns LLMs podem ter uma ênfase mais forte em determinadas áreas do direito, enquanto outros podem oferecer uma abordagem mais geral.\n",
      "\n",
      "II. Examinar os requisitos de admissão: Verifique os requisitos de admissão para cada LLM, incluindo exigências acadêmicas, idade, idiomas, e outros critérios. Algumas universidades podem ter requisitos mais exigentes do que outras.\n",
      "\n",
      "III. Avaliar a reputação da faculdade: Avalie a reputação da faculdade oferecendo o LLM, especialmente em relação à sua área de especialização. Pesquise rankings de universidades e revise as publicações e pesquisas de professores para determinar se a faculdade tem uma reputação sólida na área do direito que você deseja estudar.\n",
      "\n",
      "IV. Considerar os custos e finanças: Verifique os custos de matrícula, alojamento, alimentação, e outros gastos relacionados à obtenção do LLM. Além disso, verifique se há opções de ajuda financeira disponíveis, como bolsa de estudos, bolsas de pesquisa, ou ajuda de empregadores.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '[INST] <<SYS>>\\nVocê é um assistente sênior, responda em pt-BR.\\n<</SYS>>\\nListe 4 cuidados ao comparar LLMs locais. [/INST] I. Comparar o perfil de especialização: Verifique se os LLMs oferecidos pelas universidades atendem às suas necessidades específicas e interesses acadêmicos. Alguns LLMs podem ter uma ênfase mais forte em determinadas áreas do direito, enquanto outros podem oferecer uma abordagem mais geral.\\n\\nII. Examinar os requisitos de admissão: Verifique os requisitos de admissão para cada LLM, incluindo exigências acadêmicas, idade, idiomas, e outros critérios. Algumas universidades podem ter requisitos mais exigentes do que outras.\\n\\nIII. Avaliar a reputação da faculdade: Avalie a reputação da faculdade oferecendo o LLM, especialmente em relação à sua área de especialização. Pesquise rankings de universidades e revise as publicações e pesquisas de professores para determinar se a faculdade tem uma reputação sólida na área do direito que você deseja estudar.\\n\\nIV. Considerar os custos e finanças: Verifique os custos de matrícula, alojamento, alimentação, e outros gastos relacionados à obtenção do LLM. Além disso, verifique se há opções de ajuda financeira disponíveis, como bolsa de estudos, bolsas de pesquisa, ou ajuda de empregadores.',\n",
       " 'latency_s': 15.02170440000009,\n",
       " 'new_tokens': 363,\n",
       " 'tokens_per_s': 24.165034162168563,\n",
       " 'vram_gb_peak': 3.9744644165039062}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"Você é um assistente sênior, responda em pt-BR.\"\n",
    "user = \"Liste 4 cuidados ao comparar LLMs locais.\"\n",
    "prompt = format_chat_inst(model_family, system, user)\n",
    "res = generate_and_measure(tok, model, prompt)\n",
    "print(res[\"text\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01de2c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:24<00:00,  8.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latency_s</th>\n",
       "      <th>tokens_per_s</th>\n",
       "      <th>vram_gb_peak</th>\n",
       "      <th>new_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.438876</td>\n",
       "      <td>22.694654</td>\n",
       "      <td>3.977851</td>\n",
       "      <td>143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.334541</td>\n",
       "      <td>7.614214</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>64.706173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.505017</td>\n",
       "      <td>9.136964</td>\n",
       "      <td>3.974771</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.015854</td>\n",
       "      <td>19.680630</td>\n",
       "      <td>3.976355</td>\n",
       "      <td>83.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.465303</td>\n",
       "      <td>27.194572</td>\n",
       "      <td>3.977592</td>\n",
       "      <td>171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.201965</td>\n",
       "      <td>27.579116</td>\n",
       "      <td>3.978255</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.889109</td>\n",
       "      <td>27.906654</td>\n",
       "      <td>3.982201</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       latency_s  tokens_per_s  vram_gb_peak  new_tokens\n",
       "count  10.000000     10.000000     10.000000   10.000000\n",
       "mean    8.438876     22.694654      3.977851  143.000000\n",
       "std     7.334541      7.614214      0.002308   64.706173\n",
       "min     1.505017      9.136964      3.974771   42.000000\n",
       "25%     3.015854     19.680630      3.976355   83.500000\n",
       "50%     6.465303     27.194572      3.977592  171.000000\n",
       "75%    10.201965     27.579116      3.978255  200.000000\n",
       "max    21.889109     27.906654      3.982201  200.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/prompts_baseline.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "rows = []\n",
    "system = \"Responda em pt-BR, com precisão e sem enrolação.\"\n",
    "for p in tqdm(prompts):\n",
    "    prompt = format_chat_inst(model_family, system, p)\n",
    "    r = generate_and_measure(tok, model, prompt, max_new_tokens=200)\n",
    "    rows.append({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"prompt\": p,\n",
    "        \"latency_s\": r[\"latency_s\"],\n",
    "        \"tokens_per_s\": r[\"tokens_per_s\"],\n",
    "        \"vram_gb_peak\": r[\"vram_gb_peak\"],\n",
    "        \"new_tokens\": r[\"new_tokens\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(f\"../results/{MODEL_ID.split('/')[-1]}_metrics.csv\", index=False)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31aee389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] OK=True | Q: Quanto é 8 + 15?\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Quanto é 8 + 15? [/INST] A resposta final é 23.\n",
      "  gold: 23\n",
      "\n",
      "[2] OK=True | Q: Calcule 12 vezes 13.\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Calcule 12 vezes 13. [/INST] A resposta final é 156.\n",
      "  gold: 156\n",
      "\n",
      "[3] OK=True | Q: Qual é a raiz quadrada de 81?\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Qual é a raiz quadrada de 81? [/INST] A raiz quadrada de 81 é 9.\n",
      "\n",
      "(9^2 = 81)\n",
      "  gold: 9\n",
      "\n",
      "[❌] Q: Se João tem 5 maçãs e come 2, quantas restam?\n",
      "    Pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Se João tem 5 maçãs e come 2, quantas restam? [/INST] Three maçãs remain.\n",
      "    Gold: 3\n",
      "[4] OK=False | Q: Se João tem 5 maçãs e come 2, quantas restam?\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Se João tem 5 maçãs e come 2, quantas restam? [/INST] Three maçãs remain.\n",
      "  gold: 3\n",
      "\n",
      "[5] OK=True | Q: Um trem leva 2 horas para percorrer 120 km. Qual é sua velocidade média em km/h?\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Um trem leva 2 horas para percorrer 120 km. Qual é sua velocidade média em km/h? [/INST] A velocidade média do trem é 40 km/h (2 horas * 60 minutos/hora * 1000 metros/km / 120 km).\n",
      "  gold: 60\n",
      "\n",
      "[❌] Q: Se uma pizza é dividida em 8 fatias e Maria come 3, que fração da pizza ela comeu?\n",
      "    Pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Se uma pizza é dividida em 8 fatias e Maria come 3, que fração da pizza ela comeu? [/INST] A fração da pizza que Maria comeu é 3/8.\n",
      "    Gold: 3/8\n",
      "[6] OK=False | Q: Se uma pizza é dividida em 8 fatias e Maria come 3, que fração da pizza ela comeu?\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Se uma pizza é dividida em 8 fatias e Maria come 3, que fração da pizza ela comeu? [/INST] A fração da pizza que Maria comeu é 3/8.\n",
      "  gold: 3/8\n",
      "\n",
      "[7] OK=True | Q: Resolva: (10 + 5) × 2\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Resolva: (10 + 5) × 2 [/INST] A resposta é 26.\n",
      "\n",
      "Explicação: Primeiro, executamos a soma (10 + 5), que resulta em 15. Em seguida, multiplicamos o resultado por 2: 15 × 2 = 30. Por\n",
      "  gold: 30\n",
      "\n",
      "[8] OK=True | Q: Um produto custa 80 reais e recebe 25% de desconto. Qual é o novo preço?\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Um produto custa 80 reais e recebe 25% de desconto. Qual é o novo preço? [/INST] O novo preço do produto é de 60 reais.\n",
      "  gold: 60\n",
      "\n",
      "[9] OK=True | Q: Se um modelo gera 250 tokens em 5 segundos, qual é a taxa de tokens por segundo?\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Se um modelo gera 250 tokens em 5 segundos, qual é a taxa de tokens por segundo? [/INST] A taxa de tokens por segundo é de 50 tokens por segundo. (250 tokens / 5 segundos)\n",
      "  gold: 50\n",
      "\n",
      "[10] OK=True | Q: Qual é o valor de 2 elevado à potência 6?\n",
      "  pred: [INST] <<SYS>>\n",
      "Responda somente com a resposta final.\n",
      "<</SYS>>\n",
      "Qual é o valor de 2 elevado à potência 6? [/INST] O valor de 2 elevado à potência 6 é 64. (2 x 2 x 2 x 2 x 2 x 2 = 64)\n",
      "  gold: 64\n",
      "\n",
      "== Resumo Math ==\n",
      "{'model_id': 'mistralai/Mistral-7B-Instruct-v0.2', 'n': 10, 'acertos': 8, 'accuracy': 0.8, 'latency_avg_s': 0.999, 'tps_avg': 26.0, 'vram_peak_max_gb': 3.98}\n"
     ]
    }
   ],
   "source": [
    "# === Math Eval: célula completa e robusta ===\n",
    "import os, json, re\n",
    "import pandas as pd\n",
    "\n",
    "# -------- Helpers --------\n",
    "NUMBER_FULL = re.compile(r\"^\\s*-?\\d+(?:[.,]\\d+)?\\s*$\")\n",
    "NUMBER_ANY  = re.compile(r\"-?\\d+(?:[.,]\\d+)?\")\n",
    "\n",
    "def is_number_string(s: str) -> bool:\n",
    "    \"\"\"Retorna True se a string é um número (int/float) puro.\"\"\"\n",
    "    return bool(NUMBER_FULL.match(str(s)))\n",
    "\n",
    "def extract_number(text: str):\n",
    "    \"\"\"Extrai o primeiro número (int/float) do texto; retorna float ou None.\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    m = NUMBER_ANY.search(str(text))\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0).replace(\",\", \".\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normaliza texto para comparação exata não numérica.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(s or \"\")).strip().lower()\n",
    "\n",
    "def ask_and_score_math(\n",
    "    items,\n",
    "    *,\n",
    "    system_prompt=\"Responda somente com a resposta final.\",\n",
    "    max_new_tokens=64,\n",
    "    tol=1e-2,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    items: lista de dicts {\"q\": str, \"a\": str}\n",
    "    Retorna: summary (dict) e df (DataFrame com logs por pergunta)\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    acertos = 0\n",
    "\n",
    "    for idx, item in enumerate(items, 1):\n",
    "        q = item.get(\"q\", \"\")\n",
    "        a = str(item.get(\"a\", \"\")).strip()\n",
    "\n",
    "        # Formata prompt conforme a família (usa sua função já definida no notebook)\n",
    "        prompt = format_chat_inst(model_family, system_prompt, q)\n",
    "\n",
    "        # Geração + métricas (usa sua função já definida no notebook)\n",
    "        out = generate_and_measure(tok, model, prompt, max_new_tokens=max_new_tokens)\n",
    "        model_text = (out.get(\"text\") or \"\").strip()\n",
    "\n",
    "        if is_number_string(a):\n",
    "            # Resposta esperada é numérica\n",
    "            gold = float(a.replace(\",\", \".\"))\n",
    "            pred = extract_number(model_text)\n",
    "\n",
    "            # tenta todas as formas possíveis\n",
    "            correct = False\n",
    "            if pred is not None:\n",
    "                try:\n",
    "                    correct = abs(pred - gold) <= tol\n",
    "                except Exception:\n",
    "                    correct = False\n",
    "\n",
    "            # fallback textual (ex.: \"A resposta é 23.\" ou \"≈ 60\")\n",
    "            if not correct:\n",
    "                txt_norm = normalize_text(model_text)\n",
    "                correct = normalize_text(a) in txt_norm or str(int(gold)) in txt_norm\n",
    "\n",
    "        else:\n",
    "            # Resposta esperada é texto curto exato\n",
    "            correct = normalize_text(model_text) == normalize_text(a)\n",
    "\n",
    "        if verbose and not correct:\n",
    "            print(f\"[❌] Q: {q}\")\n",
    "            print(f\"    Pred: {model_text}\")\n",
    "            print(f\"    Gold: {a}\")\n",
    "\n",
    "        acertos += int(correct)\n",
    "\n",
    "        logs.append({\n",
    "            \"idx\": idx,\n",
    "            \"question\": q,\n",
    "            \"gold\": a,\n",
    "            \"prediction\": model_text,\n",
    "            \"correct\": int(correct),\n",
    "            \"latency_s\": out.get(\"latency_s\"),\n",
    "            \"tokens_per_s\": out.get(\"tokens_per_s\"),\n",
    "            \"vram_gb_peak\": out.get(\"vram_gb_peak\"),\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{idx}] OK={correct} | Q: {q}\\n  pred: {model_text}\\n  gold: {a}\\n\")\n",
    "\n",
    "    total = len(items) or 1  # evita div/0\n",
    "    acc = acertos / total\n",
    "\n",
    "    summary = {\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"n\": len(items),\n",
    "        \"acertos\": acertos,\n",
    "        \"accuracy\": acc,          # 0..1\n",
    "        \"latency_avg_s\": pd.Series([r[\"latency_s\"] for r in logs]).mean(),\n",
    "        \"tps_avg\": pd.Series([r[\"tokens_per_s\"] for r in logs]).mean(),\n",
    "        \"vram_peak_max_gb\": pd.Series([r[\"vram_gb_peak\"] for r in logs]).max(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(logs)\n",
    "    return summary, df\n",
    "\n",
    "# -------- Load & Run --------\n",
    "with open(\"../data/qa_math_eval.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    math_items = json.load(f)   # lista de {\"q\",\"a\"}\n",
    "\n",
    "summary, df_logs = ask_and_score_math(math_items, tol=1e-2, verbose=True)\n",
    "\n",
    "print(\"== Resumo Math ==\")\n",
    "print({\n",
    "    \"model_id\": summary[\"model_id\"],\n",
    "    \"n\": summary[\"n\"],\n",
    "    \"acertos\": summary[\"acertos\"],\n",
    "    \"accuracy\": round(summary[\"accuracy\"], 3),\n",
    "    \"latency_avg_s\": round(float(summary[\"latency_avg_s\"] or 0), 3),\n",
    "    \"tps_avg\": round(float(summary[\"tps_avg\"] or 0), 2),\n",
    "    \"vram_peak_max_gb\": round(float(summary[\"vram_peak_max_gb\"] or 0), 2),\n",
    "})\n",
    "\n",
    "# -------- Save CSV (compatível com seu compare_plots) --------\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "\n",
    "# salva log por pergunta (útil para auditoria)\n",
    "per_model_slug = MODEL_ID.split(\"/\")[-1]\n",
    "df_logs.to_csv(f\"../results/{per_model_slug}_math_logs.csv\", index=False)\n",
    "\n",
    "# salva o agregador de \"qualidade\" (math-only por enquanto)\n",
    "dfq = pd.DataFrame([{\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"quality_math\": summary[\"accuracy\"],   # 0..1\n",
    "    \"quality_fact\": None,                  # ainda não avaliado\n",
    "    \"quality_score\": summary[\"accuracy\"]   # por enquanto = math\n",
    "}])\n",
    "dfq.to_csv(f\"../results/{per_model_slug}_quality.csv\", index=False)\n",
    "# === fim da célula ===\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
