{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65110483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\small-models-benchmark\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "VRAM (GB): 23.99\n"
     ]
    }
   ],
   "source": [
    "# utils comuns\n",
    "import os, time, json, re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"VRAM (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1024**3, 2))\n",
    "\n",
    "def load_bnb_4bit(model_id: str):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "def format_chat_inst(model_family: str, system: str, user: str):\n",
    "    \"\"\"formata o prompt de acordo com o modelo.\"\"\"\n",
    "    if model_family == \"mistral\":\n",
    "        return f\"<s>[INST] <<SYS>>\\n{system}\\n<</SYS>>\\n{user} [/INST]\"\n",
    "    elif model_family == \"gemma\":\n",
    "        return f\"<start_of_turn>system\\n{system}<end_of_turn>\\n<start_of_turn>user\\n{user}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "    elif model_family == \"phi3\":\n",
    "        # phi-3 segue estilo chat simples; esse formato funciona bem\n",
    "        return f\"<s>[INST] {system}\\n\\n{user} [/INST]\"\n",
    "    else:\n",
    "        return user  # fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd998bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_and_measure(tok, model, prompt, max_new_tokens=750, temperature=0.7, top_p=0.9):\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # sincroniza antes e depois para medir latência corretamente\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.perf_counter() - t0\n",
    "\n",
    "    txt = tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    # mede VRAM pico\n",
    "    vram_peak = None\n",
    "    if device == \"cuda\":\n",
    "        vram_peak = torch.cuda.max_memory_allocated() / (1024**3)  # GB\n",
    "\n",
    "    # tokens gerados\n",
    "    new_tokens = out.shape[-1] - inputs[\"input_ids\"].shape[-1]\n",
    "    tps = new_tokens / dt if dt > 0 else float(\"inf\")\n",
    "\n",
    "    return {\"text\": txt, \"latency_s\": dt, \"new_tokens\": new_tokens, \"tokens_per_s\": tps, \"vram_gb_peak\": vram_peak}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf3713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\small-models-benchmark\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in I:\\models\\hub\\models--google--gemma-2-2b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:53<00:00, 26.86s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "token = os.environ[\"HF_TOKEN\"] = \"\"\n",
    "MODEL_ID = \"google/gemma-2-2b-it\"\n",
    "tok, model = load_bnb_4bit(MODEL_ID)\n",
    "model_family = \"gemma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb2bea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "Você é um assistente técnico em pt-BR, responda de forma concisa.\n",
      "user\n",
      "O que é um oceano de dirac?\n",
      "model\n",
      "Um oceano de Dirac é um conceito teórico de física quântica, similar a um oceano, mas em um nível quântico. \n",
      "\n",
      "**Em termos simples:**\n",
      "\n",
      "* É um modelo que descreve um estado quântico onde partículas podem existir em múltiplos estados ao mesmo tempo.\n",
      "* É um conceito abstrato, mas que pode ser usado para entender a natureza quântica de objetos como elétrons e prótons.\n",
      "\n",
      "**Para entender melhor:**\n",
      "\n",
      "* O conceito de um oceano de Dirac é parte de um modelo chamado de \"espaço de Hilbert\".\n",
      "* A física quântica é complexa e, nesse contexto, os conceitos de \"oceano\" e \"dirac\" são ferramentas para entender essa complexidade.\n",
      "\n",
      "**Em resumo:**\n",
      "\n",
      "* Um oceano de Dirac é um conceito teórico que descreve um estado quântico onde partículas podem existir em múltiplos estados ao mesmo tempo. \n",
      "* É um modelo abstrato, mas que pode ser usado para entender a natureza quântica de objetos como elétrons e prótons. \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'system\\nVocê é um assistente técnico em pt-BR, responda de forma concisa.\\nuser\\nO que é um oceano de dirac?\\nmodel\\nUm oceano de Dirac é um conceito teórico de física quântica, similar a um oceano, mas em um nível quântico. \\n\\n**Em termos simples:**\\n\\n* É um modelo que descreve um estado quântico onde partículas podem existir em múltiplos estados ao mesmo tempo.\\n* É um conceito abstrato, mas que pode ser usado para entender a natureza quântica de objetos como elétrons e prótons.\\n\\n**Para entender melhor:**\\n\\n* O conceito de um oceano de Dirac é parte de um modelo chamado de \"espaço de Hilbert\".\\n* A física quântica é complexa e, nesse contexto, os conceitos de \"oceano\" e \"dirac\" são ferramentas para entender essa complexidade.\\n\\n**Em resumo:**\\n\\n* Um oceano de Dirac é um conceito teórico que descreve um estado quântico onde partículas podem existir em múltiplos estados ao mesmo tempo. \\n* É um modelo abstrato, mas que pode ser usado para entender a natureza quântica de objetos como elétrons e prótons. \\n\\n\\n\\n',\n",
       " 'latency_s': 8.311798499999895,\n",
       " 'new_tokens': 212,\n",
       " 'tokens_per_s': 25.505911867329637,\n",
       " 'vram_gb_peak': 2.1279845237731934}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"Você é um assistente técnico em pt-BR, responda de forma concisa.\"\n",
    "user = \"O que é um oceano de dirac?\"\n",
    "prompt = format_chat_inst(model_family, system, user)\n",
    "res = generate_and_measure(tok, model, prompt)\n",
    "print(res[\"text\"])\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb0e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.76s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latency_s</th>\n",
       "      <th>tokens_per_s</th>\n",
       "      <th>vram_gb_peak</th>\n",
       "      <th>new_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.756823</td>\n",
       "      <td>27.252107</td>\n",
       "      <td>2.129904</td>\n",
       "      <td>103.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.829659</td>\n",
       "      <td>0.672756</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>78.45657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.660180</td>\n",
       "      <td>25.750568</td>\n",
       "      <td>2.128388</td>\n",
       "      <td>17.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.187695</td>\n",
       "      <td>27.225455</td>\n",
       "      <td>2.128959</td>\n",
       "      <td>32.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.207152</td>\n",
       "      <td>27.527196</td>\n",
       "      <td>2.129531</td>\n",
       "      <td>86.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.742808</td>\n",
       "      <td>27.640919</td>\n",
       "      <td>2.130803</td>\n",
       "      <td>187.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.253135</td>\n",
       "      <td>27.898012</td>\n",
       "      <td>2.132290</td>\n",
       "      <td>200.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       latency_s  tokens_per_s  vram_gb_peak  new_tokens\n",
       "count  10.000000     10.000000     10.000000    10.00000\n",
       "mean    3.756823     27.252107      2.129904   103.10000\n",
       "std     2.829659      0.672756      0.001293    78.45657\n",
       "min     0.660180     25.750568      2.128388    17.00000\n",
       "25%     1.187695     27.225455      2.128959    32.50000\n",
       "50%     3.207152     27.527196      2.129531    86.00000\n",
       "75%     6.742808     27.640919      2.130803   187.25000\n",
       "max     7.253135     27.898012      2.132290   200.00000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/prompts_baseline.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "rows = []\n",
    "system = \"Responda em pt-BR, com precisão e sem enrolação.\"\n",
    "for p in tqdm(prompts):\n",
    "    prompt = format_chat_inst(model_family, system, p)\n",
    "    r = generate_and_measure(tok, model, prompt, max_new_tokens=200)\n",
    "    rows.append({\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"prompt\": p,\n",
    "        \"latency_s\": r[\"latency_s\"],\n",
    "        \"tokens_per_s\": r[\"tokens_per_s\"],\n",
    "        \"vram_gb_peak\": r[\"vram_gb_peak\"],\n",
    "        \"new_tokens\": r[\"new_tokens\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(f\"../results/{MODEL_ID.split('/')[-1]}_metrics.csv\", index=False)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8830e3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] OK=True | Q: Quanto é 8 + 15?\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Quanto é 8 + 15?\n",
      "model\n",
      "23\n",
      "  gold: 23\n",
      "\n",
      "[2] OK=True | Q: Calcule 12 vezes 13.\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Calcule 12 vezes 13.\n",
      "model\n",
      "156\n",
      "  gold: 156\n",
      "\n",
      "[3] OK=True | Q: Qual é a raiz quadrada de 81?\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Qual é a raiz quadrada de 81?\n",
      "model\n",
      "9\n",
      "  gold: 9\n",
      "\n",
      "[4] OK=True | Q: Se João tem 5 maçãs e come 2, quantas restam?\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Se João tem 5 maçãs e come 2, quantas restam?\n",
      "model\n",
      "3\n",
      "  gold: 3\n",
      "\n",
      "[5] OK=True | Q: Um trem leva 2 horas para percorrer 120 km. Qual é sua velocidade média em km/h?\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Um trem leva 2 horas para percorrer 120 km. Qual é sua velocidade média em km/h?\n",
      "model\n",
      "60 km/h\n",
      "  gold: 60\n",
      "\n",
      "[❌] Q: Se uma pizza é dividida em 8 fatias e Maria come 3, que fração da pizza ela comeu?\n",
      "    Pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Se uma pizza é dividida em 8 fatias e Maria come 3, que fração da pizza ela comeu?\n",
      "model\n",
      "3/8\n",
      "    Gold: 3/8\n",
      "[6] OK=False | Q: Se uma pizza é dividida em 8 fatias e Maria come 3, que fração da pizza ela comeu?\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Se uma pizza é dividida em 8 fatias e Maria come 3, que fração da pizza ela comeu?\n",
      "model\n",
      "3/8\n",
      "  gold: 3/8\n",
      "\n",
      "[❌] Q: Resolva: (10 + 5) × 2\n",
      "    Pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Resolva: (10 + 5) × 2\n",
      "model\n",
      "20\n",
      "    Gold: 30\n",
      "[7] OK=False | Q: Resolva: (10 + 5) × 2\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Resolva: (10 + 5) × 2\n",
      "model\n",
      "20\n",
      "  gold: 30\n",
      "\n",
      "[❌] Q: Um produto custa 80 reais e recebe 25% de desconto. Qual é o novo preço?\n",
      "    Pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Um produto custa 80 reais e recebe 25% de desconto. Qual é o novo preço?\n",
      "model\n",
      "56,00 reais.\n",
      "    Gold: 60\n",
      "[8] OK=False | Q: Um produto custa 80 reais e recebe 25% de desconto. Qual é o novo preço?\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Um produto custa 80 reais e recebe 25% de desconto. Qual é o novo preço?\n",
      "model\n",
      "56,00 reais.\n",
      "  gold: 60\n",
      "\n",
      "[9] OK=True | Q: Se um modelo gera 250 tokens em 5 segundos, qual é a taxa de tokens por segundo?\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Se um modelo gera 250 tokens em 5 segundos, qual é a taxa de tokens por segundo?\n",
      "model\n",
      "50 tokens/segundo.\n",
      "  gold: 50\n",
      "\n",
      "[10] OK=True | Q: Qual é o valor de 2 elevado à potência 6?\n",
      "  pred: system\n",
      "Responda somente com a resposta final.\n",
      "user\n",
      "Qual é o valor de 2 elevado à potência 6?\n",
      "model\n",
      "64\n",
      "  gold: 64\n",
      "\n",
      "== Resumo Math ==\n",
      "{'model_id': 'google/gemma-2-2b-it', 'n': 10, 'acertos': 7, 'accuracy': 0.7, 'latency_avg_s': 0.25, 'tps_avg': 24.18, 'vram_peak_max_gb': 2.13}\n"
     ]
    }
   ],
   "source": [
    "# === Math Eval: célula completa e robusta ===\n",
    "import os, json, re\n",
    "import pandas as pd\n",
    "\n",
    "# -------- Helpers --------\n",
    "NUMBER_FULL = re.compile(r\"^\\s*-?\\d+(?:[.,]\\d+)?\\s*$\")\n",
    "NUMBER_ANY  = re.compile(r\"-?\\d+(?:[.,]\\d+)?\")\n",
    "\n",
    "def is_number_string(s: str) -> bool:\n",
    "    \"\"\"Retorna True se a string é um número (int/float) puro.\"\"\"\n",
    "    return bool(NUMBER_FULL.match(str(s)))\n",
    "\n",
    "def extract_number(text: str):\n",
    "    \"\"\"Extrai o primeiro número (int/float) do texto; retorna float ou None.\"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    m = NUMBER_ANY.search(str(text))\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return float(m.group(0).replace(\",\", \".\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Normaliza texto para comparação exata não numérica.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(s or \"\")).strip().lower()\n",
    "\n",
    "def ask_and_score_math(\n",
    "    items,\n",
    "    *,\n",
    "    system_prompt=\"Responda somente com a resposta final.\",\n",
    "    max_new_tokens=64,\n",
    "    tol=1e-2,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    items: lista de dicts {\"q\": str, \"a\": str}\n",
    "    Retorna: summary (dict) e df (DataFrame com logs por pergunta)\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "    acertos = 0\n",
    "\n",
    "    for idx, item in enumerate(items, 1):\n",
    "        q = item.get(\"q\", \"\")\n",
    "        a = str(item.get(\"a\", \"\")).strip()\n",
    "\n",
    "        # Formata prompt conforme a família (usa sua função já definida no notebook)\n",
    "        prompt = format_chat_inst(model_family, system_prompt, q)\n",
    "\n",
    "        # Geração + métricas (usa sua função já definida no notebook)\n",
    "        out = generate_and_measure(tok, model, prompt, max_new_tokens=max_new_tokens)\n",
    "        model_text = (out.get(\"text\") or \"\").strip()\n",
    "\n",
    "        if is_number_string(a):\n",
    "            # Resposta esperada é numérica\n",
    "            gold = float(a.replace(\",\", \".\"))\n",
    "            pred = extract_number(model_text)\n",
    "\n",
    "            # tenta todas as formas possíveis\n",
    "            correct = False\n",
    "            if pred is not None:\n",
    "                try:\n",
    "                    correct = abs(pred - gold) <= tol\n",
    "                except Exception:\n",
    "                    correct = False\n",
    "\n",
    "            # fallback textual (ex.: \"A resposta é 23.\" ou \"≈ 60\")\n",
    "            if not correct:\n",
    "                txt_norm = normalize_text(model_text)\n",
    "                correct = normalize_text(a) in txt_norm or str(int(gold)) in txt_norm\n",
    "\n",
    "        else:\n",
    "            # Resposta esperada é texto curto exato\n",
    "            correct = normalize_text(model_text) == normalize_text(a)\n",
    "\n",
    "        if verbose and not correct:\n",
    "            print(f\"[❌] Q: {q}\")\n",
    "            print(f\"    Pred: {model_text}\")\n",
    "            print(f\"    Gold: {a}\")\n",
    "\n",
    "        acertos += int(correct)\n",
    "\n",
    "        logs.append({\n",
    "            \"idx\": idx,\n",
    "            \"question\": q,\n",
    "            \"gold\": a,\n",
    "            \"prediction\": model_text,\n",
    "            \"correct\": int(correct),\n",
    "            \"latency_s\": out.get(\"latency_s\"),\n",
    "            \"tokens_per_s\": out.get(\"tokens_per_s\"),\n",
    "            \"vram_gb_peak\": out.get(\"vram_gb_peak\"),\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{idx}] OK={correct} | Q: {q}\\n  pred: {model_text}\\n  gold: {a}\\n\")\n",
    "\n",
    "    total = len(items) or 1  # evita div/0\n",
    "    acc = acertos / total\n",
    "\n",
    "    summary = {\n",
    "        \"model_id\": MODEL_ID,\n",
    "        \"n\": len(items),\n",
    "        \"acertos\": acertos,\n",
    "        \"accuracy\": acc,          # 0..1\n",
    "        \"latency_avg_s\": pd.Series([r[\"latency_s\"] for r in logs]).mean(),\n",
    "        \"tps_avg\": pd.Series([r[\"tokens_per_s\"] for r in logs]).mean(),\n",
    "        \"vram_peak_max_gb\": pd.Series([r[\"vram_gb_peak\"] for r in logs]).max(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(logs)\n",
    "    return summary, df\n",
    "\n",
    "# -------- Load & Run --------\n",
    "with open(\"../data/qa_math_eval.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    math_items = json.load(f)   # lista de {\"q\",\"a\"}\n",
    "\n",
    "summary, df_logs = ask_and_score_math(math_items, tol=1e-2, verbose=True)\n",
    "\n",
    "print(\"== Resumo Math ==\")\n",
    "print({\n",
    "    \"model_id\": summary[\"model_id\"],\n",
    "    \"n\": summary[\"n\"],\n",
    "    \"acertos\": summary[\"acertos\"],\n",
    "    \"accuracy\": round(summary[\"accuracy\"], 3),\n",
    "    \"latency_avg_s\": round(float(summary[\"latency_avg_s\"] or 0), 3),\n",
    "    \"tps_avg\": round(float(summary[\"tps_avg\"] or 0), 2),\n",
    "    \"vram_peak_max_gb\": round(float(summary[\"vram_peak_max_gb\"] or 0), 2),\n",
    "})\n",
    "\n",
    "# -------- Save CSV (compatível com seu compare_plots) --------\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "\n",
    "# salva log por pergunta (útil para auditoria)\n",
    "per_model_slug = MODEL_ID.split(\"/\")[-1]\n",
    "df_logs.to_csv(f\"../results/{per_model_slug}_math_logs.csv\", index=False)\n",
    "\n",
    "# salva o agregador de \"qualidade\" (math-only por enquanto)\n",
    "dfq = pd.DataFrame([{\n",
    "    \"model_id\": MODEL_ID,\n",
    "    \"quality_math\": summary[\"accuracy\"],   # 0..1\n",
    "    \"quality_fact\": None,                  # ainda não avaliado\n",
    "    \"quality_score\": summary[\"accuracy\"]   # por enquanto = math\n",
    "}])\n",
    "dfq.to_csv(f\"../results/{per_model_slug}_quality.csv\", index=False)\n",
    "# === fim da célula ===\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
