# ğŸ§  Local LLM Bench
**Benchmark prÃ¡tico e reprodutÃ­vel de LLMs locais (Gemma, Phi-3, Mistral) em uma GPU RTX 4090**

> Fiz um repositÃ³rio base com testes para LLM locais. Meu objetivo era medir o desempenho delas tanto em recurso utilizados quanto em qualidade.    
Todos os modelos sÃ£o bem leves, entÃ£o apesar do harware mais potente que eu usei, eles podem ser reproduzidos atÃ© mesmo em notebook com uma placa de vÃ­deo dedicada e pelo menos 16GB de memÃ³ria ram. No final temos um panorama interessante dos modelos e desempenho.  
Minha avaliaÃ§Ã£o Ã© no sentido de verificar se esses modelos seriam adequados para utilizaÃ§Ã£o aplicaÃ§Ãµes web e locais sem a necessidade de conexÃ£o com a internet para acessos a modelos mais completos, dessa forma preservando os dados do usuÃ¡rio e evitando possiveis problemas com legislaÃ§Ãµes de dados pessoais.  

---

## ğŸš€ VisÃ£o Geral

Este projeto avalia o **desempenho real** de modelos de linguagem locais, medindo:
- â±ï¸ **LatÃªncia mÃ©dia** (tempo por resposta)  
- âš¡ **Tokens gerados por segundo**  
- ğŸ’¾ **Uso de VRAM (GB)**  
- ğŸ¯ **AcurÃ¡cia matemÃ¡tica bÃ¡sica** (respostas numÃ©ricas objetivas)

Tudo rodando localmente via **Transformers + bitsandbytes (4-bit)**.

Modelos testados:
| Modelo | ParÃ¢metros | Contexto | QuantizaÃ§Ã£o | Link |
|---------|-------------|-----------|--------------|------|
| `google/gemma-2-2b-it` | 2B | 8k | 4-bit | [Hugging Face](https://huggingface.co/google/gemma-2-2b-it) |
| `microsoft/Phi-3-mini-4k-instruct` | 3.8B | 4k | 4-bit | [Hugging Face](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) |
| `mistralai/Mistral-7B-Instruct-v0.2` | 7B | 32k | 4-bit | [Hugging Face](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) |

---

## ğŸ§© Estrutura do RepositÃ³rio

```
ğŸ“¦ local-llm-bench/
â”œâ”€ notebooks/
â”‚  â”œâ”€ gemma2_2b.ipynb
â”‚  â”œâ”€ phi3_mini.ipynb
â”‚  â”œâ”€ mistral7b.ipynb
â”‚  â””â”€ compare_plots.ipynb   â† anÃ¡lise e grÃ¡ficos
â”œâ”€ data/
â”‚  â”œâ”€ prompts_baseline.json
â”‚  â””â”€ qa_math_eval.json
â”œâ”€ results/
â”‚  â”œâ”€ *_metrics.csv         â† tempo / vram / throughput
â”‚  â”œâ”€ *_math_logs.csv       â† logs por pergunta
â”‚  â””â”€ *_quality.csv         â† qualidade mÃ©dia
â””â”€ README.md
```

---

## âš™ï¸ Como Reproduzir

### 1ï¸âƒ£ Instale dependÃªncias
```bash
pip install -r requirements.txt
```
**VersÃ£o utilizada da biblioteca CUDA: 12.8**

### 2ï¸âƒ£ Baixe os modelos automaticamente
Cada notebook baixa o modelo da Hugging Face, Ã© necessÃ¡ria a chave de conexÃ£o do hugging face
para download dos modelos.



### 3ï¸âƒ£ Rode os notebooks
Execute:
- `notebooks/gemma2_2b.ipynb`
- `notebooks/phi3_mini.ipynb`
- `notebooks/mistral7b.ipynb`

Cada um salva suas mÃ©tricas e logs em `/results`.

### 4ï¸âƒ£ Gere os grÃ¡ficos comparativos
Execute:
```bash
notebooks/compare_plots.ipynb
```

---

## ğŸ“Š GrÃ¡ficos Gerados

| MÃ©trica | DescriÃ§Ã£o |
|----------|------------|
| **LatÃªncia mÃ©dia (s)** | tempo mÃ©dio de resposta por prompt |
| **Tokens/s** | taxa mÃ©dia de geraÃ§Ã£o |
| **VRAM (GB)** | pico de memÃ³ria usado |
| **Qualidade** | acurÃ¡cia nas 10 perguntas matemÃ¡ticas |
| **Radar global** | visÃ£o consolidada de trade-offs |
| **EficiÃªncia** | (Qualidade Ã— Tokens/s) Ã· VRAM |

### ğŸ“Š Exemplos de grÃ¡ficos
### Tokens
<p align="center">
  <img src="assets/tokens.png" alt="Tokens por segundo" width="500"/>
</p>

### Vram
![Pico de vram utilizada](assets/vram.png)

### Comparativo de qualidade/vram
![Qualidade](assets/vramqualidade.png)
---

## ğŸ’¡ ObservaÃ§Ãµes

- Todos os testes foram executados em **RTX 4090 (24GB VRAM)**.  
- Modelos carregados em **4-bit (bitsandbytes)**.  
- As perguntas e prompts sÃ£o neutras e pÃºblicas (`/data/`).  
- O objetivo Ã© **medir eficiÃªncia**, nÃ£o apenas desempenho bruto.

---

## ğŸ§  Resultados Gerais (exemplo)

| Modelo | Tokens/s | VRAM (GB) | Qualidade | EficiÃªncia |
|---------|-----------|-----------|------------|-------------|
| Phi-3 Mini | 39 | 2.2 | 0.90 | ğŸ¥‡ |
| Gemma 2B | 33 | 1.9 | 0.85 | ğŸ¥ˆ |
| Mistral 7B | 22 | 10.3 | 0.92 | ğŸ¥‰ |

---

## âœï¸ Sobre o Projeto

Projeto desenvolvido por **[Marcelo Pepis](https://www.linkedin.com/in/marcelo-pepis/)**  

---

## ğŸ¤ ContribuiÃ§Ãµes

Pull requests sÃ£o bem-vindos!  
Se quiser incluir novos modelos (ex.: Qwen, TinyLlama, Hermes), siga o padrÃ£o:
- `notebooks/<model>.ipynb`
- `results/<model>_metrics.csv`
- `results/<model>_quality.csv`

---

## ğŸ“„ LicenÃ§a
MIT â€” utilize livremente com atribuiÃ§Ã£o.
